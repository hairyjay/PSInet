{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wav2vec-experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIWh8SRRI75J"
      },
      "source": [
        "\n",
        "#  Wav2Vec Experiment\n",
        "https://github.com/pytorch/fairseq/tree/master/examples/wav2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhXybiGFBO8N"
      },
      "source": [
        "## 1 prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1mGrO9JQ8MB",
        "outputId": "0d9be75c-7f5e-4741-9206-817ee70bc065",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install soundfile\n",
        "!git clone https://github.com/pytorch/fairseq.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Installing collected packages: soundfile\n",
            "Successfully installed soundfile-0.10.3.post1\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 19688 (delta 2), reused 7 (delta 2), pack-reused 19677\u001b[K\n",
            "Receiving objects: 100% (19688/19688), 8.86 MiB | 9.46 MiB/s, done.\n",
            "Resolving deltas: 100% (14701/14701), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJxfmbkmSv_e",
        "outputId": "e43e1e46-6788-4fbe-a2dd-f11004c70a75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd fairseq/\n",
        "!pip install --editable ./"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fairseq\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+3c5647c) (1.7.0+cu101)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+3c5647c) (0.29.21)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+3c5647c) (0.5.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+3c5647c) (0.7)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+3c5647c) (1.14.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+3c5647c) (2019.12.20)\n",
            "Collecting hydra-core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/03/fee705ef16675a103d8e929255f5fa0ee79432ac38bafad6935d6ad170f9/hydra_core-1.0.3-py3-none-any.whl (122kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 14.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+3c5647c) (4.41.1)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+3c5647c) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq==1.0.0a0+3c5647c) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->fairseq==1.0.0a0+3c5647c) (3.7.4.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq==1.0.0a0+3c5647c) (2.20)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 20.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq==1.0.0a0+3c5647c) (3.3.0)\n",
            "Collecting omegaconf>=2.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq==1.0.0a0+3c5647c) (3.4.0)\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 11.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime, PyYAML\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp36-none-any.whl size=141230 sha256=b8b8260ec3bc113d7522e051f55a8afe4a87c4aa0cadb27abfb91f93b3f32157\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=b53e09899daa443b643749d831f72ef86bb8600095d7a62893bb1ada7a698cc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built antlr4-python3-runtime PyYAML\n",
            "Installing collected packages: antlr4-python3-runtime, PyYAML, omegaconf, hydra-core, portalocker, sacrebleu, fairseq\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed PyYAML-5.3.1 antlr4-python3-runtime-4.8 fairseq hydra-core-1.0.3 omegaconf-2.0.5 portalocker-2.0.0 sacrebleu-1.4.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nw9B79cqIwM"
      },
      "source": [
        "### Download Data: WSJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFyxnDCGqK-g",
        "outputId": "574e592d-0bb6-4d7c-c039-ec8c72f6a068",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k--gxy7oqVYo"
      },
      "source": [
        "!mkdir /content/WSJ\n",
        "!cp /content/drive/My\\ Drive/project/csr_1_LDC93S6A.tgz /content/WSJ/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tAYgKoytn_J",
        "outputId": "516bae86-1547-492f-d780-a5ce076635dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/WSJ/\n",
        "!tar -xzf /content/WSJ/csr_1_LDC93S6A.tgz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/WSJ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReZ7S9LkZTvT"
      },
      "source": [
        "### Download Data: Librispeech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcsqe9iDbLbL"
      },
      "source": [
        "Download data for the first time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyxoD7EwZVbK",
        "outputId": "51d18968-b3d3-4568-8ea8-4d1ed9fda6d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# %cd /content\n",
        "# !wget http://www.openslr.org/resources/12/train-clean-100.tar.gz\n",
        "# !wget http://www.openslr.org/resources/12/dev-clean.tar.gz\n",
        "# !wget http://www.openslr.org/resources/12/test-clean.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "--2020-11-11 03:49:22--  http://www.openslr.org/resources/12/train-clean-100.tar.gz\n",
            "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
            "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6387309499 (5.9G) [application/x-gzip]\n",
            "Saving to: ‘train-clean-100.tar.gz’\n",
            "\n",
            "train-clean-100.tar 100%[===================>]   5.95G  59.4MB/s    in 88s     \n",
            "\n",
            "2020-11-11 03:50:50 (68.8 MB/s) - ‘train-clean-100.tar.gz’ saved [6387309499/6387309499]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nga-KgIsiFQ"
      },
      "source": [
        "# copy the dataset to my drive\n",
        "# !cp train-clean-100.tar.gz /content/drive/My\\ Drive/project/\n",
        "# !cp dev-clean.tar.gz /content/drive/My\\ Drive/project/\n",
        "# !cp test-clean.tar.gz /content/drive/My\\ Drive/project/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjXuTc1-jemp"
      },
      "source": [
        "copy data over drive to local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tct63KXlsC6s"
      },
      "source": [
        "!cp /content/drive/My\\ Drive/project/train-clean-100.tar.gz /content/\n",
        "!cp /content/drive/My\\ Drive/project/dev-clean.tar.gz /content/\n",
        "!cp /content/drive/My\\ Drive/project/test-clean.tar.gz /content/"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_zz9WTkcMxg",
        "outputId": "cebc9609-1e61-4994-f91d-91727ad1e0be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "\n",
        "#unzip files\n",
        "!tar -xzf train-clean-100.tar.gz\n",
        "!tar -xzf dev-clean.tar.gz\n",
        "!tar -xzf test-clean.tar.gz\n",
        "\n",
        "%cd /content/\n",
        "!rm train-clean-100.tar.gz dev-clean.tar.gz test-clean.tar.gz"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgQK73mcb2dz"
      },
      "source": [
        "## 2 pretrain task - Prepare training data manifest:\n",
        "Given a directory containing wav files to be used for pretraining (we recommend splitting each file into separate file 10 to 30 seconds in length)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDDk7Xg2jhFl",
        "outputId": "801ce985-42b7-4268-a300-1fe682e707a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir /content/manifest\n",
        "!mkdir /content/manifest/train\n",
        "!mkdir /content/manifest/dev\n",
        "\n",
        "%cd /content/fairseq\n",
        "!python examples/wav2vec/wav2vec_manifest.py \"/content/LibriSpeech/train-clean-100\" \\\n",
        "--dest \"/content/manifest/train/\" --ext \"flac\" --valid-percent 0\n",
        "\n",
        "!python examples/wav2vec/wav2vec_manifest.py \"/content/LibriSpeech/dev-clean\" \\\n",
        "--dest \"/content/manifest/dev/\" --ext \"flac\" --valid-percent 1\n",
        "\n",
        "!ls -l /content/manifest/train\n",
        "!ls -l /content/manifest/dev\n",
        "!mv /content/manifest/dev/valid.tsv /content/manifest/train/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fairseq\n",
            "total 1104\n",
            "-rw-r--r-- 1 root root 1122737 Nov 14 07:31 train.tsv\n",
            "-rw-r--r-- 1 root root      37 Nov 14 07:31 valid.tsv\n",
            "total 108\n",
            "-rw-r--r-- 1 root root     31 Nov 14 07:31 train.tsv\n",
            "-rw-r--r-- 1 root root 106202 Nov 14 07:31 valid.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAEfxwZVlkRV"
      },
      "source": [
        "### Train a wav2vec 2.0 base model (pretrain task):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyluh_S7kQT1",
        "outputId": "d3ba73a7-c03a-44ea-bff9-2251d96a9040",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir /content/model\n",
        "%cd /content/fairseq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fairseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkAkT2SlsCEc"
      },
      "source": [
        "!pip install omegaconf\n",
        "!pip install hydra-core --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMXRyBg8xGiI"
      },
      "source": [
        "# --update-freq 64 --distributed-world-size 64 --distributed-port 8888  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuFJAPoGjpR6"
      },
      "source": [
        "!python train.py \"/content/manifest/train\" \\\n",
        "--save-dir \"/content/model\" --fp16 --num-workers 6 --task audio_pretraining --criterion wav2vec --arch wav2vec2 \\\n",
        "--log-keys '[\"prob_perplexity\",\"code_perplexity\",\"temp\"]' --quantize-targets --extractor-mode default \\\n",
        "--conv-feature-layers '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] * 2' --final-dim 256 --latent-vars 320 \\\n",
        "--latent-groups 2 --latent-temp '(2,0.5,0.999995)' --infonce --optimizer adam \\\n",
        "--adam-betas '(0.9,0.98)' --adam-eps 1e-06 --lr-scheduler polynomial_decay --total-num-update 400000 \\\n",
        "--lr 0.0005 --warmup-updates 32000 --mask-length 10 --mask-prob 0.65 --mask-selection static --mask-other 0 \\\n",
        "--encoder-layerdrop 0.05 --dropout-input 0.1 --dropout-features 0.1 --feature-grad-mult 0.1 \\\n",
        "--loss-weights '[0.1, 10]' --conv-pos 128 --conv-pos-groups 16 --num-negatives 100 --cross-sample-negatives 0 \\\n",
        "--max-sample-size 250000 --min-sample-size 32000 --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
        "--max-tokens 1400000 --max-update 400000 --skip-invalid-size-inputs-valid-test --ddp-backend no_c10d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBMDoNju0mlD"
      },
      "source": [
        "## 3 Fine-tune task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0eQFUDvrsHB",
        "outputId": "ef08c6be-0e1c-4143-aef7-283fd21c7395",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/manifest\n",
        "!mkdir fine-tune"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/manifest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQDbfTUIAk58"
      },
      "source": [
        "### 3.1 generates labels for the Librispeech dataset \n",
        "from the tsv file produced by wav2vec_manifest.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ullVR2bN0niO",
        "outputId": "b07db798-7dae-4c9a-9f27-2b2588a8b6a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/fairseq/examples/wav2vec\n",
        "!python libri_labels.py /content/manifest/train/train.tsv \\\n",
        "--output-dir /content/manifest/train/ --output-name \"train\"\n",
        "\n",
        "!python libri_labels.py /content/manifest/train/valid.tsv \\\n",
        "--output-dir /content/manifest/train/ --output-name \"valid\""
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fairseq/examples/wav2vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p7y_t51BvrG"
      },
      "source": [
        "### 3.2 download pretrained wav2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psJHTwacsjfV",
        "outputId": "370970d9-cff9-4223-fe39-986839ae5060",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# download pretrained model\n",
        "!mkdir /content/pretrain_model\n",
        "%cd /content/pretrain_model\n",
        "!wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/libri960_big.pt"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-14 07:41:23--  https://dl.fbaipublicfiles.com/fairseq/wav2vec/libri960_big.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3173903620 (3.0G) [application/octet-stream]\n",
            "Saving to: ‘libri960_big.pt’\n",
            "\n",
            "libri960_big.pt     100%[===================>]   2.96G  10.8MB/s    in 4m 58s  \n",
            "\n",
            "2020-11-14 07:46:21 (10.2 MB/s) - ‘libri960_big.pt’ saved [3173903620/3173903620]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJF6O0MXFXv-"
      },
      "source": [
        "### [optional] 3.3 TO-DO: build wav2letter python bindings\n",
        "https://github.com/facebookresearch/wav2letter/wiki/Building-Python-bindings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB5Tgdkiv7Gp"
      },
      "source": [
        "### 3.4 download letter vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZWk44Hw0KEZ"
      },
      "source": [
        "%cd /content/manifest/train\n",
        "!wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/dict.ltr.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs3-mN5h_y3q"
      },
      "source": [
        "### 3.5 run fine-tune\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k82EWqSItHFI"
      },
      "source": [
        "# save-dir\n",
        "!mkdir /content/finetune_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44i8X78lscSE",
        "outputId": "3e8c0138-8b7a-4c6f-c53e-7ecdc4cef960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/fairseq\n",
        "!python train.py --distributed-world-size 128 \"/content/manifest/train/\" --save-dir \"/content/finetune_model/\" --fp16 \\\n",
        "--post-process letter --valid-subset \"/content/manifest/train/valid\" --no-epoch-checkpoints --best-checkpoint-metric wer --num-workers 4 \\\n",
        "--max-update 80000 --sentence-avg --task audio_pretraining --arch wav2vec_ctc --w2v-path /content/pretrain_model/libri960_big.pt \\\n",
        "--labels ltr --apply-mask --mask-selection static --mask-other 0 --mask-length 10 --mask-prob 0.5 --layerdrop 0.1 \\\n",
        "--mask-channel-selection static --mask-channel-other 0 --mask-channel-length 64 --mask-channel-prob 0.5 --zero-infinity \\\n",
        "--feature-grad-mult 0.0 --freeze-finetune-updates 10000 --validate-after-updates 10000 --optimizer adam \\\n",
        "--adam-betas '(0.9, 0.98)' --adam-eps 1e-08 --lr 2e-05 --lr-scheduler tri_stage --warmup-steps 8000 --hold-steps 32000 \\\n",
        "--decay-steps 40000 --final-lr-scale 0.05 --final-dropout 0.0 --dropout 0.0 --activation-dropout 0.1 --criterion ctc \\\n",
        "--attention-dropout 0.0 --max-tokens 1280000 --seed 2337 --log-format json --log-interval 500 --ddp-backend no_c10d"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fairseq\n",
            "2020-11-14 08:13:56 | INFO | fairseq_cli.train | {'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 500, 'log_format': 'json', 'tensorboard_logdir': None, 'wandb_project': None, 'seed': 2337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'letter', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'local_rank': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'broadcast_buffers': False, 'distributed_wrapper': 'DDP', 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 4, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1280000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': '/content/manifest/train/valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 10000, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1280000, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [1], 'lr': [2e-05], 'min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/finetune_model/', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': False, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='wav2vec_ctc', activation_dropout=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, all_gather_list_size=16384, apply_mask=True, arch='wav2vec_ctc', attention_dropout=0.0, autoregressive=False, batch_size=None, batch_size_valid=None, beam=5, best_checkpoint_metric='wer', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, constraints=None, cpu=False, criterion='ctc', curriculum=0, data='/content/manifest/train/', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decay_steps=40000, decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, dropout=0.0, dropout_input=0, empty_cache_freq=0, enable_padding=False, eos=2, eval_wer=False, eval_wer_post_process='letter', eval_wer_tokenizer=None, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.0, final_lr_scale=0.05, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, freeze_finetune_updates=10000, gen_subset='test', hold_steps=32000, init_lr_scale=0.01, iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, labels='ltr', layerdrop=0.1, lenpen=1, lm_path=None, lm_weight=0.0, local_rank=0, localsgd_frequency=3, log_format='json', log_interval=500, lr=[2e-05], lr_scheduler='tri_stage', mask_channel_length=64, mask_channel_other=0.0, mask_channel_prob=0.5, mask_channel_selection='static', mask_length=10, mask_other=0.0, mask_prob=0.5, mask_selection='static', match_source_len=False, max_epoch=0, max_len_a=0, max_len_b=200, max_sample_size=None, max_tokens=1280000, max_tokens_valid=1280000, max_update=80000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_lr=-1.0, min_sample_size=None, model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_pretrained_weights=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, normalize=False, nprocs_per_node=1, num_shards=1, num_workers=4, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, post_process='letter', prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=False, sample_rate=16000, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='/content/finetune_model/', save_interval=1, save_interval_updates=0, score_reference=False, scoring='bleu', seed=2337, sentence_avg=True, shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_time_hours=0, task='audio_pretraining', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', unk=3, unkpen=0, unnormalized=False, update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='/content/manifest/train/valid', validate_after_updates=10000, validate_interval=1, validate_interval_updates=0, w2v_path='/content/pretrain_model/libri960_big.pt', wandb_project=None, warmup_steps=8000, weight_decay=0.0, wer_args=None, zero_infinity=True, zero_sharding='none'), 'task': {'_name': 'audio_pretraining', 'data': '/content/manifest/train/', 'labels': 'ltr', 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': None, 'min_sample_size': None, 'eval_wer': False, 'eval_wer_config': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': False, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_wer_tokenizer': None, 'eval_wer_post_process': 'letter', 'autoregressive': False}, 'criterion': Namespace(_name='ctc', activation_dropout=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, all_gather_list_size=16384, apply_mask=True, arch='wav2vec_ctc', attention_dropout=0.0, autoregressive=False, batch_size=None, batch_size_valid=None, beam=5, best_checkpoint_metric='wer', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, constraints=None, cpu=False, criterion='ctc', curriculum=0, data='/content/manifest/train/', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decay_steps=40000, decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, dropout=0.0, dropout_input=0, empty_cache_freq=0, enable_padding=False, eos=2, eval_wer=False, eval_wer_post_process='letter', eval_wer_tokenizer=None, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.0, final_lr_scale=0.05, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, freeze_finetune_updates=10000, gen_subset='test', hold_steps=32000, init_lr_scale=0.01, iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, labels='ltr', layerdrop=0.1, lenpen=1, lm_path=None, lm_weight=0.0, local_rank=0, localsgd_frequency=3, log_format='json', log_interval=500, lr=[2e-05], lr_scheduler='tri_stage', mask_channel_length=64, mask_channel_other=0.0, mask_channel_prob=0.5, mask_channel_selection='static', mask_length=10, mask_other=0.0, mask_prob=0.5, mask_selection='static', match_source_len=False, max_epoch=0, max_len_a=0, max_len_b=200, max_sample_size=None, max_tokens=1280000, max_tokens_valid=1280000, max_update=80000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_lr=-1.0, min_sample_size=None, model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_pretrained_weights=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, normalize=False, nprocs_per_node=1, num_shards=1, num_workers=4, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, post_process='letter', prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=False, sample_rate=16000, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='/content/finetune_model/', save_interval=1, save_interval_updates=0, score_reference=False, scoring='bleu', seed=2337, sentence_avg=True, shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_time_hours=0, task='audio_pretraining', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', unk=3, unkpen=0, unnormalized=False, update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='/content/manifest/train/valid', validate_after_updates=10000, validate_interval=1, validate_interval_updates=0, w2v_path='/content/pretrain_model/libri960_big.pt', wandb_project=None, warmup_steps=8000, weight_decay=0.0, wer_args=None, zero_infinity=True, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [2e-05]}, 'lr_scheduler': Namespace(_name='tri_stage', activation_dropout=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, all_gather_list_size=16384, apply_mask=True, arch='wav2vec_ctc', attention_dropout=0.0, autoregressive=False, batch_size=None, batch_size_valid=None, beam=5, best_checkpoint_metric='wer', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, constraints=None, cpu=False, criterion='ctc', curriculum=0, data='/content/manifest/train/', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decay_steps=40000, decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, dropout=0.0, dropout_input=0, empty_cache_freq=0, enable_padding=False, eos=2, eval_wer=False, eval_wer_post_process='letter', eval_wer_tokenizer=None, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.0, final_lr_scale=0.05, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, freeze_finetune_updates=10000, gen_subset='test', hold_steps=32000, init_lr_scale=0.01, iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, labels='ltr', layerdrop=0.1, lenpen=1, lm_path=None, lm_weight=0.0, local_rank=0, localsgd_frequency=3, log_format='json', log_interval=500, lr=[2e-05], lr_scheduler='tri_stage', mask_channel_length=64, mask_channel_other=0.0, mask_channel_prob=0.5, mask_channel_selection='static', mask_length=10, mask_other=0.0, mask_prob=0.5, mask_selection='static', match_source_len=False, max_epoch=0, max_len_a=0, max_len_b=200, max_sample_size=None, max_tokens=1280000, max_tokens_valid=1280000, max_update=80000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_lr=-1.0, min_sample_size=None, model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_pretrained_weights=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, normalize=False, nprocs_per_node=1, num_shards=1, num_workers=4, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, post_process='letter', prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=False, sample_rate=16000, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='/content/finetune_model/', save_interval=1, save_interval_updates=0, score_reference=False, scoring='bleu', seed=2337, sentence_avg=True, shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_time_hours=0, task='audio_pretraining', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', unk=3, unkpen=0, unnormalized=False, update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='/content/manifest/train/valid', validate_after_updates=10000, validate_interval=1, validate_interval_updates=0, w2v_path='/content/pretrain_model/libri960_big.pt', wandb_project=None, warmup_steps=8000, weight_decay=0.0, wer_args=None, zero_infinity=True, zero_sharding='none'), 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}\n",
            "2020-11-14 08:13:56 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 2703, skipped 0 samples\n",
            "tcmalloc: large alloc 1269530624 bytes == 0x2b686000 @  0x7f5628874b6b 0x7f5628894379 0x7f55cd2ea74e 0x7f55cd2ec7b6 0x7f5607d55ba5 0x7f5617a101d9 0x551555 0x5a9dac 0x50a433 0x50beb4 0x507be4 0x508ec2 0x5a4c61 0x5a4fb8 0x4e012e 0x50a461 0x50beb4 0x507be4 0x588e5c 0x59fd0e 0x50d256 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x508f37\n",
            "tcmalloc: large alloc 1269530624 bytes == 0x7713e000 @  0x7f5628874b6b 0x7f5628894379 0x7f55cd2ea74e 0x7f55cd2ec7b6 0x7f5607d55ba5 0x7f5617a101d9 0x551555 0x5a9dac 0x50a433 0x50beb4 0x507be4 0x508ec2 0x5a4c61 0x5a4fb8 0x4e012e 0x50a461 0x50beb4 0x507be4 0x588e5c 0x59fd0e 0x50d256 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x508f37\n",
            "/content/fairseq/fairseq/criterions/ctc.py:21: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.\n",
            "  super().__init__(args, task)\n",
            "2020-11-14 08:14:07 | INFO | fairseq_cli.train | Wav2VecCtc(\n",
            "  (w2v_encoder): Wav2VecEncoder(\n",
            "    (w2v_model): Wav2Vec2Model(\n",
            "      (feature_extractor): ConvFeatureExtractionModel(\n",
            "        (conv_layers): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
            "            (1): Dropout(p=0.0, inplace=False)\n",
            "            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
            "            (3): GELU()\n",
            "          )\n",
            "          (1): Sequential(\n",
            "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
            "            (1): Dropout(p=0.0, inplace=False)\n",
            "            (2): GELU()\n",
            "          )\n",
            "          (2): Sequential(\n",
            "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
            "            (1): Dropout(p=0.0, inplace=False)\n",
            "            (2): GELU()\n",
            "          )\n",
            "          (3): Sequential(\n",
            "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
            "            (1): Dropout(p=0.0, inplace=False)\n",
            "            (2): GELU()\n",
            "          )\n",
            "          (4): Sequential(\n",
            "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
            "            (1): Dropout(p=0.0, inplace=False)\n",
            "            (2): GELU()\n",
            "          )\n",
            "          (5): Sequential(\n",
            "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
            "            (1): Dropout(p=0.0, inplace=False)\n",
            "            (2): GELU()\n",
            "          )\n",
            "          (6): Sequential(\n",
            "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
            "            (1): Dropout(p=0.0, inplace=False)\n",
            "            (2): GELU()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
            "      (dropout_input): Dropout(p=0.0, inplace=False)\n",
            "      (dropout_features): Dropout(p=0.1, inplace=False)\n",
            "      (quantizer): None\n",
            "      (project_q): None\n",
            "      (encoder): TransformerEncoder(\n",
            "        (pos_conv): Sequential(\n",
            "          (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
            "          (1): SamePad()\n",
            "          (2): GELU()\n",
            "        )\n",
            "        (layers): ModuleList(\n",
            "          (0): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (1): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (2): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (3): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (4): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (5): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (6): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (7): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (8): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (9): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (10): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (11): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (12): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (13): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (14): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (15): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (16): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (17): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (18): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (19): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (20): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (21): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (22): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (23): TransformerSentenceEncoderLayer(\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (final_proj): None\n",
            "    )\n",
            "    (final_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (proj): Linear(in_features=1024, out_features=32, bias=True)\n",
            "  )\n",
            ")\n",
            "2020-11-14 08:14:07 | INFO | fairseq_cli.train | task: AudioPretrainingTask\n",
            "2020-11-14 08:14:07 | INFO | fairseq_cli.train | model: Wav2VecCtc\n",
            "2020-11-14 08:14:07 | INFO | fairseq_cli.train | criterion: CtcCriterion)\n",
            "2020-11-14 08:14:07 | INFO | fairseq_cli.train | num. model params: 315461792 (num. trained: 315461792)\n",
            "2020-11-14 08:14:19 | INFO | fairseq.trainer | detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias\n",
            "2020-11-14 08:14:19 | INFO | fairseq.trainer | detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias\n",
            "2020-11-14 08:14:19 | INFO | fairseq.trainer | detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias\n",
            "2020-11-14 08:14:19 | INFO | fairseq.trainer | detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias\n",
            "2020-11-14 08:14:19 | INFO | fairseq.trainer | detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias\n",
            "2020-11-14 08:14:19 | INFO | fairseq.trainer | detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias\n",
            "2020-11-14 08:14:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2020-11-14 08:14:19 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    \n",
            "2020-11-14 08:14:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2020-11-14 08:14:19 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2020-11-14 08:14:19 | INFO | fairseq_cli.train | max tokens per GPU = 1280000 and batch size per GPU = None\n",
            "2020-11-14 08:14:19 | INFO | fairseq.trainer | no existing checkpoint found /content/finetune_model/checkpoint_last.pt\n",
            "2020-11-14 08:14:19 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2020-11-14 08:14:19 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 28539, skipped 0 samples\n",
            "2020-11-14 08:14:19 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2020-11-14 08:14:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n",
            "2020-11-14 08:14:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n",
            "2020-11-14 08:14:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
            "2020-11-14 08:14:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\n",
            "2020-11-14 08:14:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0\n",
            "2020-11-14 08:15:56 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.101, \"loss\": \"2180.02\", \"ntokens\": \"1059.49\", \"nsentences\": \"5.56\", \"nll_loss\": \"11.44\", \"wps\": \"5535.1\", \"ups\": \"5.22\", \"wpb\": \"1059.5\", \"bsz\": \"5.6\", \"num_updates\": \"500\", \"lr\": \"1.4375e-06\", \"gnorm\": \"3337.6\", \"loss_scale\": \"4\", \"train_wall\": \"95\", \"wall\": \"98\"}\n",
            "2020-11-14 08:17:31 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.2, \"loss\": \"2011.83\", \"ntokens\": \"1053.77\", \"nsentences\": \"5.688\", \"nll_loss\": \"10.859\", \"wps\": \"5527.4\", \"ups\": \"5.25\", \"wpb\": \"1053.8\", \"bsz\": \"5.7\", \"num_updates\": \"1000\", \"lr\": \"2.675e-06\", \"gnorm\": \"3238.08\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"193\"}\n",
            "2020-11-14 08:19:07 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.3, \"loss\": \"1859.85\", \"ntokens\": \"1052.7\", \"nsentences\": \"5.622\", \"nll_loss\": \"9.933\", \"wps\": \"5538.5\", \"ups\": \"5.26\", \"wpb\": \"1052.7\", \"bsz\": \"5.6\", \"num_updates\": \"1500\", \"lr\": \"3.9125e-06\", \"gnorm\": \"2891.28\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"288\"}\n",
            "2020-11-14 08:20:42 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.399, \"loss\": \"1641.83\", \"ntokens\": \"1051.77\", \"nsentences\": \"5.694\", \"nll_loss\": \"8.888\", \"wps\": \"5527.2\", \"ups\": \"5.26\", \"wpb\": \"1051.8\", \"bsz\": \"5.7\", \"num_updates\": \"2000\", \"lr\": \"5.15e-06\", \"gnorm\": \"2107.82\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"383\"}\n",
            "2020-11-14 08:22:17 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.499, \"loss\": \"1484.81\", \"ntokens\": \"1051.05\", \"nsentences\": \"5.706\", \"nll_loss\": \"8.061\", \"wps\": \"5509.8\", \"ups\": \"5.24\", \"wpb\": \"1051.1\", \"bsz\": \"5.7\", \"num_updates\": \"2500\", \"lr\": \"6.3875e-06\", \"gnorm\": \"1102.65\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"479\"}\n",
            "2020-11-14 08:23:52 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.599, \"loss\": \"1451.56\", \"ntokens\": \"1054.15\", \"nsentences\": \"5.67\", \"nll_loss\": \"7.808\", \"wps\": \"5523\", \"ups\": \"5.24\", \"wpb\": \"1054.2\", \"bsz\": \"5.7\", \"num_updates\": \"3000\", \"lr\": \"7.625e-06\", \"gnorm\": \"535.803\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"574\"}\n",
            "2020-11-14 08:25:28 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.698, \"loss\": \"1406.03\", \"ntokens\": \"1054.17\", \"nsentences\": \"5.698\", \"nll_loss\": \"7.6\", \"wps\": \"5539.6\", \"ups\": \"5.25\", \"wpb\": \"1054.2\", \"bsz\": \"5.7\", \"num_updates\": \"3500\", \"lr\": \"8.8625e-06\", \"gnorm\": \"460.641\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"669\"}\n",
            "2020-11-14 08:27:03 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.798, \"loss\": \"1399.81\", \"ntokens\": \"1064.31\", \"nsentences\": \"5.612\", \"nll_loss\": \"7.381\", \"wps\": \"5572.8\", \"ups\": \"5.24\", \"wpb\": \"1064.3\", \"bsz\": \"5.6\", \"num_updates\": \"4000\", \"lr\": \"1.01e-05\", \"gnorm\": \"430.026\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"765\"}\n",
            "2020-11-14 08:28:39 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.897, \"loss\": \"1293.5\", \"ntokens\": \"1055.66\", \"nsentences\": \"5.862\", \"nll_loss\": \"7.183\", \"wps\": \"5527.4\", \"ups\": \"5.24\", \"wpb\": \"1055.7\", \"bsz\": \"5.9\", \"num_updates\": \"4500\", \"lr\": \"1.13375e-05\", \"gnorm\": \"413.805\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"860\"}\n",
            "2020-11-14 08:30:14 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.997, \"loss\": \"1305.42\", \"ntokens\": \"1056.97\", \"nsentences\": \"5.754\", \"nll_loss\": \"7.107\", \"wps\": \"5539.8\", \"ups\": \"5.24\", \"wpb\": \"1057\", \"bsz\": \"5.8\", \"num_updates\": \"5000\", \"lr\": \"1.2575e-05\", \"gnorm\": \"404.415\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"956\"}\n",
            "2020-11-14 08:30:17 | INFO | fairseq_cli.train | begin validation on \"/content/manifest/train/valid\" subset\n",
            "2020-11-14 08:31:25 | INFO | /content/manifest/train/valid | {\"epoch\": 1, \"/content/manifest/train/valid_loss\": \"837.171\", \"/content/manifest/train/valid_ntokens\": \"970.53\", \"/content/manifest/train/valid_nsentences\": \"9.01\", \"/content/manifest/train/valid_nll_loss\": \"7.772\", \"/content/manifest/train/valid_uer\": \"99.066\", \"/content/manifest/train/valid_wer\": \"99.998\", \"/content/manifest/train/valid_raw_wer\": \"99.998\", \"/content/manifest/train/valid_wps\": \"4275.7\", \"/content/manifest/train/valid_wpb\": \"970.5\", \"/content/manifest/train/valid_bsz\": \"9\", \"/content/manifest/train/valid_num_updates\": \"5015\"}\n",
            "2020-11-14 08:31:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2020-11-14 08:31:44 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/finetune_model/checkpoint_best.pt (epoch 1 @ 5015 updates, score 99.998) (writing took 18.657125335000273 seconds)\n",
            "2020-11-14 08:31:44 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2020-11-14 08:31:44 | INFO | train | {\"epoch\": 1, \"train_loss\": \"1600.04\", \"train_ntokens\": \"1055.49\", \"train_nsentences\": \"5.68554\", \"train_nll_loss\": \"8.619\", \"train_wps\": \"5071.8\", \"train_ups\": \"4.81\", \"train_wpb\": \"1055.5\", \"train_bsz\": \"5.7\", \"train_num_updates\": \"5015\", \"train_lr\": \"1.26121e-05\", \"train_gnorm\": \"1488.9\", \"train_loss_scale\": \"4\", \"train_train_wall\": \"935\", \"train_wall\": \"1046\"}\n",
            "2020-11-14 08:31:44 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2020-11-14 08:33:23 | INFO | train_inner | {\"epoch\": 2, \"update\": 1.097, \"loss\": \"1338.49\", \"ntokens\": \"1056.63\", \"nsentences\": \"5.52\", \"nll_loss\": \"6.992\", \"wps\": \"2794.3\", \"ups\": \"2.64\", \"wpb\": \"1056.6\", \"bsz\": \"5.5\", \"num_updates\": \"5500\", \"lr\": \"1.38125e-05\", \"gnorm\": \"397.253\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"1145\"}\n",
            "2020-11-14 08:34:59 | INFO | train_inner | {\"epoch\": 2, \"update\": 1.196, \"loss\": \"1221.09\", \"ntokens\": \"1053.94\", \"nsentences\": \"5.866\", \"nll_loss\": \"6.796\", \"wps\": \"5502.5\", \"ups\": \"5.22\", \"wpb\": \"1053.9\", \"bsz\": \"5.9\", \"num_updates\": \"6000\", \"lr\": \"1.505e-05\", \"gnorm\": \"367.067\", \"loss_scale\": \"4\", \"train_wall\": \"94\", \"wall\": \"1240\"}\n",
            "2020-11-14 08:36:35 | INFO | train_inner | {\"epoch\": 2, \"update\": 1.296, \"loss\": \"1276.64\", \"ntokens\": \"1053.6\", \"nsentences\": \"5.434\", \"nll_loss\": \"6.584\", \"wps\": \"5502.1\", \"ups\": \"5.22\", \"wpb\": \"1053.6\", \"bsz\": \"5.4\", \"num_updates\": \"6500\", \"lr\": \"1.62875e-05\", \"gnorm\": \"354.788\", \"loss_scale\": \"4\", \"train_wall\": \"94\", \"wall\": \"1336\"}\n",
            "2020-11-14 08:38:10 | INFO | train_inner | {\"epoch\": 2, \"update\": 1.395, \"loss\": \"1262.68\", \"ntokens\": \"1056.69\", \"nsentences\": \"5.54\", \"nll_loss\": \"6.62\", \"wps\": \"5536.8\", \"ups\": \"5.24\", \"wpb\": \"1056.7\", \"bsz\": \"5.5\", \"num_updates\": \"7000\", \"lr\": \"1.7525e-05\", \"gnorm\": \"358.532\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"1432\"}\n",
            "2020-11-14 08:39:45 | INFO | train_inner | {\"epoch\": 2, \"update\": 1.495, \"loss\": \"1186.32\", \"ntokens\": \"1057.2\", \"nsentences\": \"5.712\", \"nll_loss\": \"6.41\", \"wps\": \"5539.1\", \"ups\": \"5.24\", \"wpb\": \"1057.2\", \"bsz\": \"5.7\", \"num_updates\": \"7500\", \"lr\": \"1.87625e-05\", \"gnorm\": \"331.709\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"1527\"}\n",
            "2020-11-14 08:41:21 | INFO | train_inner | {\"epoch\": 2, \"update\": 1.595, \"loss\": \"1178.43\", \"ntokens\": \"1057.19\", \"nsentences\": \"5.712\", \"nll_loss\": \"6.367\", \"wps\": \"5519.4\", \"ups\": \"5.22\", \"wpb\": \"1057.2\", \"bsz\": \"5.7\", \"num_updates\": \"8000\", \"lr\": \"2e-05\", \"gnorm\": \"337.069\", \"loss_scale\": \"4\", \"train_wall\": \"94\", \"wall\": \"1623\"}\n",
            "2020-11-14 08:42:56 | INFO | train_inner | {\"epoch\": 2, \"update\": 1.694, \"loss\": \"1202.01\", \"ntokens\": \"1047.45\", \"nsentences\": \"5.58\", \"nll_loss\": \"6.403\", \"wps\": \"5523.7\", \"ups\": \"5.27\", \"wpb\": \"1047.5\", \"bsz\": \"5.6\", \"num_updates\": \"8500\", \"lr\": \"2e-05\", \"gnorm\": \"348.591\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"1718\"}\n",
            "2020-11-14 08:44:31 | INFO | train_inner | {\"epoch\": 2, \"update\": 1.794, \"loss\": \"1110.83\", \"ntokens\": \"1053.68\", \"nsentences\": \"5.972\", \"nll_loss\": \"6.296\", \"wps\": \"5536.7\", \"ups\": \"5.25\", \"wpb\": \"1053.7\", \"bsz\": \"6\", \"num_updates\": \"9000\", \"lr\": \"2e-05\", \"gnorm\": \"326.653\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"1813\"}\n",
            "2020-11-14 08:46:06 | INFO | train_inner | {\"epoch\": 2, \"update\": 1.893, \"loss\": \"1102.57\", \"ntokens\": \"1060.67\", \"nsentences\": \"5.828\", \"nll_loss\": \"6.058\", \"wps\": \"5583.4\", \"ups\": \"5.26\", \"wpb\": \"1060.7\", \"bsz\": \"5.8\", \"num_updates\": \"9500\", \"lr\": \"2e-05\", \"gnorm\": \"292.333\", \"loss_scale\": \"4\", \"train_wall\": \"93\", \"wall\": \"1908\"}\n",
            "2020-11-14 08:47:41 | INFO | train_inner | {\"epoch\": 2, \"update\": 1.993, \"loss\": \"1120.42\", \"ntokens\": \"1057.99\", \"nsentences\": \"5.698\", \"nll_loss\": \"6.034\", \"wps\": \"5593.5\", \"ups\": \"5.29\", \"wpb\": \"1058\", \"bsz\": \"5.7\", \"num_updates\": \"10000\", \"lr\": \"2e-05\", \"gnorm\": \"302.67\", \"loss_scale\": \"4\", \"train_wall\": \"92\", \"wall\": \"2002\"}\n",
            "2020-11-14 08:47:57 | INFO | fairseq_cli.train | begin validation on \"/content/manifest/train/valid\" subset\n",
            "2020-11-14 08:49:05 | INFO | /content/manifest/train/valid | {\"epoch\": 2, \"/content/manifest/train/valid_loss\": \"818.187\", \"/content/manifest/train/valid_ntokens\": \"970.53\", \"/content/manifest/train/valid_nsentences\": \"9.01\", \"/content/manifest/train/valid_nll_loss\": \"7.596\", \"/content/manifest/train/valid_uer\": \"99.08\", \"/content/manifest/train/valid_wer\": \"99.998\", \"/content/manifest/train/valid_raw_wer\": \"99.998\", \"/content/manifest/train/valid_wps\": \"4314\", \"/content/manifest/train/valid_wpb\": \"970.5\", \"/content/manifest/train/valid_bsz\": \"9\", \"/content/manifest/train/valid_num_updates\": \"10035\", \"/content/manifest/train/valid_best_wer\": \"99.998\"}\n",
            "2020-11-14 08:49:05 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2020-11-14 08:50:24 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/finetune_model/checkpoint_best.pt (epoch 2 @ 10035 updates, score 99.998) (writing took 79.1087284840005 seconds)\n",
            "2020-11-14 08:50:24 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2020-11-14 08:50:24 | INFO | train | {\"epoch\": 2, \"train_loss\": \"1197.48\", \"train_ntokens\": \"1055.45\", \"train_nsentences\": \"5.68506\", \"train_nll_loss\": \"6.45\", \"train_wps\": \"4732\", \"train_ups\": \"4.48\", \"train_wpb\": \"1055.4\", \"train_bsz\": \"5.7\", \"train_num_updates\": \"10035\", \"train_lr\": \"2e-05\", \"train_gnorm\": \"346.934\", \"train_loss_scale\": \"4\", \"train_train_wall\": \"945\", \"train_wall\": \"2165\"}\n",
            "2020-11-14 08:50:24 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2020-11-14 08:51:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.18 GiB (GPU 0; 15.75 GiB total capacity; 5.88 GiB already allocated; 962.88 MiB free; 13.60 GiB reserved in total by PyTorch)\n",
            "2020-11-14 08:51:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1            |        cudaMalloc retries: 8         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6023 MB |   14341 MB |  174702 GB |  174696 GB |\n",
            "|       from large pool |    6018 MB |   14336 MB |  174624 GB |  174618 GB |\n",
            "|       from small pool |       5 MB |       8 MB |      78 GB |      78 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6023 MB |   14341 MB |  174702 GB |  174696 GB |\n",
            "|       from large pool |    6018 MB |   14336 MB |  174624 GB |  174618 GB |\n",
            "|       from small pool |       5 MB |       8 MB |      78 GB |      78 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13924 MB |   14864 MB |   30678 MB |   16754 MB |\n",
            "|       from large pool |   13918 MB |   14854 MB |   30636 MB |   16718 MB |\n",
            "|       from small pool |       6 MB |      10 MB |      42 MB |      36 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    7900 MB |    7901 MB |  331949 GB |  331942 GB |\n",
            "|       from large pool |    7899 MB |    7899 MB |  331848 GB |  331840 GB |\n",
            "|       from small pool |       0 MB |       4 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     754    |     862    |   13252 K  |   13251 K  |\n",
            "|       from large pool |     295    |     481    |    7054 K  |    7054 K  |\n",
            "|       from small pool |     459    |     475    |    6197 K  |    6196 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     754    |     862    |   13252 K  |   13251 K  |\n",
            "|       from large pool |     295    |     481    |    7054 K  |    7054 K  |\n",
            "|       from small pool |     459    |     475    |    6197 K  |    6196 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     142    |     247    |     184    |\n",
            "|       from large pool |      60    |     137    |     226    |     166    |\n",
            "|       from small pool |       3    |       5    |      21    |      18    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |     138    |    5555 K  |    5555 K  |\n",
            "|       from large pool |      48    |     116    |    3084 K  |    3084 K  |\n",
            "|       from small pool |      21    |      55    |    2471 K  |    2471 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2020-11-14 08:51:18 | ERROR | fairseq.trainer | OOM during optimization, irrecoverable\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 14, in <module>\n",
            "    cli_main()\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 392, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/content/fairseq/fairseq/distributed_utils.py\", line 334, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 130, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 219, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 672, in train_step\n",
            "    raise e\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 648, in train_step\n",
            "    self.optimizer.step()\n",
            "  File \"/content/fairseq/fairseq/optim/fp16_optimizer.py\", line 209, in step\n",
            "    self.fp32_optimizer.step(closure)\n",
            "  File \"/content/fairseq/fairseq/optim/fairseq_optimizer.py\", line 119, in step\n",
            "    self.optimizer.step(closure)\n",
            "  File \"/content/fairseq/fairseq/optim/adam.py\", line 210, in step\n",
            "    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 1.18 GiB (GPU 0; 15.75 GiB total capacity; 5.88 GiB already allocated; 962.88 MiB free; 13.60 GiB reserved in total by PyTorch)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0-lEzo2qRht"
      },
      "source": [
        "## 4 TODO: Evaluation a CTC model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKespOHz6SQl"
      },
      "source": [
        "Fairseq transformer language model used in the wav2vec 2.0 paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh_yEn6j5VkZ",
        "outputId": "52cf495f-2be3-4e59-b5fb-27a41d38513b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# language model\n",
        "%cd /content/LM\n",
        "!wget https://dl.fbaipublicfiles.com/wav2letter/sota/2019/lm/lm_librispeech_word_transformer.pt\n",
        "!wget https://dl.fbaipublicfiles.com/wav2letter/sota/2019/lm/lm_librispeech_word_transformer.dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LM\n",
            "--2020-11-10 16:03:40--  https://dl.fbaipublicfiles.com/wav2letter/sota/2019/lm/lm_librispeech_word_transformer.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3374337651 (3.1G) [binary/octet-stream]\n",
            "Saving to: ‘lm_librispeech_word_transformer.pt’\n",
            "\n",
            "lm_librispeech_word 100%[===================>]   3.14G  23.0MB/s    in 2m 29s  \n",
            "\n",
            "2020-11-10 16:06:09 (21.6 MB/s) - ‘lm_librispeech_word_transformer.pt’ saved [3374337651/3374337651]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u25DFK2U6I9Z"
      },
      "source": [
        "Letter dictionary for pre-trained models can be found here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7d2qTnr1F3V",
        "outputId": "010f942a-15d8-48e5-c43a-80b3763de5e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "python /content/fairseq/examples/speech_recognition/infer.py /checkpoint/abaevski/data/speech/libri/10h/wav2vec/raw --task audio_pretraining \\\n",
        "--nbest 1 --path /content/model/ --gen-subset \"dev_other\" --results-path /content/model/ --w2l-decoder kenlm \\\n",
        "--lm-model /path/to/kenlm.bin --lm-weight 2 --world-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 \\\n",
        "--post-process letter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-2bb63d936157>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python /content/fairseq/examples/speech_recognition/infer.py /checkpoint/abaevski/data/speech/libri/10h/wav2vec/raw --task audio_pretraining --nbest 1 --path /content/model/ --gen-subset \"dev_other\" --results-path /content/model/ --w2l-decoder kenlm --lm-model /path/to/kenlm.bin --lm-weight 2 --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 --post-process letter\u001b[0m\n\u001b[0m                                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGjGAhe1t7Be"
      },
      "source": [
        "Wav2Letter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysujeYmF9ZEj",
        "outputId": "d7e4e5ba-0242-404e-bbd9-f488bf2b30dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/facebookresearch/wav2letter.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'wav2letter'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 6215 (delta 11), reused 28 (delta 9), pack-reused 6174\u001b[K\n",
            "Receiving objects: 100% (6215/6215), 5.98 MiB | 22.69 MiB/s, done.\n",
            "Resolving deltas: 100% (3990/3990), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBG7y8VR9b8v",
        "outputId": "ac176558-478c-460c-cd80-7ae2b1281e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd wav2letter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/wav2letter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKZ6tS_z94s9",
        "outputId": "7fdb772b-0749-44da-d6f9-4de61e3992e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!cd bindings/python\n",
        "!pip install -e ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: cd: bindings/python: No such file or directory\n",
            "\u001b[31mERROR: File \"setup.py\" not found. Directory cannot be installed in editable mode: /content/wav2letter\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD3ONwRC95xp",
        "outputId": "16ecc096-3d50-4dfe-e405-ace07bd226ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!export KENLM_ROOT_DIR=/root/kenlm && \\\n",
        "cd /root/wav2letter/bindings/python && pip install -e .\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: cd: /root/wav2letter/bindings/python: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T_bdEng_Mzk",
        "outputId": "3841882d-f4f0-4640-926a-afe173ce0ad4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install packaging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPqkKzx1-Ae6",
        "outputId": "5c00b1c9-2195-4cca-e22a-6d62e859d233",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir build && cd build && cmake .. && make -j8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- OpenMP found\n",
            "CMake Error at CMakeLists.txt:25 (find_package):\n",
            "  By not providing \"FindArrayFire.cmake\" in CMAKE_MODULE_PATH this project\n",
            "  has asked CMake to find a package configuration file provided by\n",
            "  \"ArrayFire\", but CMake did not find one.\n",
            "\n",
            "  Could not find a package configuration file provided by \"ArrayFire\"\n",
            "  (requested version 3.7.1) with any of the following names:\n",
            "\n",
            "    ArrayFireConfig.cmake\n",
            "    arrayfire-config.cmake\n",
            "\n",
            "  Add the installation prefix of \"ArrayFire\" to CMAKE_PREFIX_PATH or set\n",
            "  \"ArrayFire_DIR\" to a directory containing one of the above files.  If\n",
            "  \"ArrayFire\" provides a separate development package or SDK, be sure it has\n",
            "  been installed.\n",
            "\n",
            "\n",
            "-- Configuring incomplete, errors occurred!\n",
            "See also \"/content/wav2letter/build/CMakeFiles/CMakeOutput.log\".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpNiwwhu-e8v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjYuke9ovA45"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}