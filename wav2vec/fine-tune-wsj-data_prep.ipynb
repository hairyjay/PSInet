{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/project/notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librispeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/ubuntu/project/manifest_libri’: File exists\n",
      "mkdir: cannot create directory ‘/home/ubuntu/project/manifest_libri/train’: File exists\n",
      "mkdir: cannot create directory ‘/home/ubuntu/project/manifest_libri/dev’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir /home/ubuntu/project/manifest_libri\n",
    "!mkdir /home/ubuntu/project/manifest_libri/train\n",
    "!mkdir /home/ubuntu/project/manifest_libri/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fairseq\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python examples/wav2vec/wav2vec_manifest.py \"/home/ubuntu/data/LibriSpeech/train-clean-100\" \\\n",
    "--dest \"/home/ubuntu/project/manifest_libri/train\" --ext \"flac\" --valid-percent 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python examples/wav2vec/wav2vec_manifest.py \"/home/ubuntu/data/LibriSpeech/dev-clean\" \\\n",
    "--dest \"/home/ubuntu/project/manifest/dev/\" --ext \"flac\" --valid-percent 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/project/manifest/train\n",
      "--2020-12-04 05:54:40--  https://dl.fbaipublicfiles.com/fairseq/wav2vec/dict.ltr.txt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 207 [text/plain]\n",
      "Saving to: ‘dict.ltr.txt.1’\n",
      "\n",
      "dict.ltr.txt.1      100%[===================>]     207  --.-KB/s    in 0s      \n",
      "\n",
      "2020-12-04 05:54:41 (48.6 MB/s) - ‘dict.ltr.txt.1’ saved [207/207]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/project/manifest/train/\n",
    "!wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/dict.ltr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fairseq/examples/wav2vec\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/fairseq/examples/wav2vec\n",
    "!python libri_labels.py /home/ubuntu/project/manifest_libri/train/train.tsv \\\n",
    "--output-dir /home/ubuntu/project/manifest/train/ --output-name train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/ubuntu/project/manifest_libri/dev/valid.tsv': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!mv /home/ubuntu/project/manifest_libri/dev/valid.tsv /home/ubuntu/project/manifest_libri/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer scp files to real wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/project/notebook\n"
     ]
    }
   ],
   "source": [
    "%cp \"/home/ubuntu/espnet/tools/sph2pipe_v2.5/sph2pipe\" /home/ubuntu/project/notebook/\n",
    "%cd /home/ubuntu/project/notebook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10/330 [00:00<00:03, 96.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of waves = 330 in test_eval92_5k\n",
      "start converting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330/330 [00:03<00:00, 97.44it/s]\n",
      "  2%|▏         | 10/503 [00:00<00:04, 98.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish converting test_eval92_5k\n",
      "number of waves = 503 in test_dev93\n",
      "start converting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 503/503 [00:05<00:00, 98.42it/s]\n",
      "  3%|▎         | 10/333 [00:00<00:03, 99.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish converting test_dev93\n",
      "number of waves = 333 in test_eval92\n",
      "start converting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 333/333 [00:03<00:00, 98.46it/s]\n",
      "  5%|▍         | 10/215 [00:00<00:02, 98.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish converting test_eval92\n",
      "number of waves = 215 in test_eval93_5k\n",
      "start converting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [00:02<00:00, 98.19it/s]\n",
      "  1%|          | 10/913 [00:00<00:09, 98.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish converting test_eval93_5k\n",
      "number of waves = 913 in dev_dt_05\n",
      "start converting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 913/913 [00:09<00:00, 98.42it/s]\n",
      "  5%|▍         | 10/213 [00:00<00:02, 98.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish converting dev_dt_05\n",
      "number of waves = 213 in test_eval93\n",
      "start converting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213/213 [00:02<00:00, 98.54it/s]\n",
      "  2%|▏         | 10/503 [00:00<00:04, 98.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish converting test_eval93\n",
      "number of waves = 503 in dev_dt_20\n",
      "start converting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 503/503 [00:05<00:00, 98.27it/s]\n",
      "  2%|▏         | 10/513 [00:00<00:05, 98.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish converting dev_dt_20\n",
      "number of waves = 513 in test_dev93_5k\n",
      "start converting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 513/513 [00:05<00:00, 98.33it/s]\n",
      "  0%|          | 10/37416 [00:00<06:21, 98.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish converting test_dev93_5k\n",
      "number of waves = 37416 in train_si284\n",
      "start converting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37416/37416 [06:23<00:00, 97.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish converting train_si284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "orig_wsj_data_root = \"/home/ubuntu/espnet/egs2/wsj/asr1/data/\"\n",
    "datasets = os.listdir(wsj_data_root)\n",
    "wsj_data_root = \"/home/ubuntu/data/WSJ\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "for subset in datasets:\n",
    "    if subset not in ['local', 'nlsyms.txt']:\n",
    "        f = open(orig_wsj_data_root + subset + \"/wav.scp\", 'r')\n",
    "        wavs = f.readlines()\n",
    "        f.close()\n",
    "        os.system(f\"mkdir {wsj_data_root}/{subset}\")\n",
    "        print(\"number of waves = {} in {}\".format(len(wavs), subset))\n",
    "        print(\"start converting\")\n",
    "        for wav in tqdm(wavs):\n",
    "            parts = wav.split(\" \")\n",
    "            file_path = parts[-2]\n",
    "            name=file_path.split(\"/\")[-1]\n",
    "            save_file = wsj_data_root+\"/\"+subset+\"/\"+name+\".wav\"\n",
    "            os.system(f\"./sph2pipe -f rif {path} {save_file}\")\n",
    "        print(f\"finish converting {subset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fairseq\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/fairseq\n",
    "!python examples/wav2vec/wav2vec_manifest.py \"/home/ubuntu/data/WSJ/train_si284\" \\\n",
    "--dest \"/home/ubuntu/project/manifest/train/\" --ext \"wav\" --valid-percent 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/WSJ/train_si284\r\n",
      "4asc031c.wv1.wav\t207299\r\n",
      "4a6c030h.wv1.wav\t207299\r\n",
      "019c020k.wv1.wav\t207299\r\n",
      "40nc020q.wv1.wav\t207299\r\n",
      "47tc020x.wv1.wav\t207299\r\n",
      "48sc0203.wv1.wav\t207299\r\n",
      "4a2c0309.wv1.wav\t207299\r\n",
      "01kc020d.wv1.wav\t207299\r\n",
      "4b1c020l.wv1.wav\t207299\r\n"
     ]
    }
   ],
   "source": [
    "!head /home/ubuntu/project/manifest/train/train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fairseq\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/fairseq\n",
    "!python examples/wav2vec/wav2vec_manifest.py \"/home/ubuntu/data/WSJ/test_dev93/\" \\\n",
    "--dest \"/home/ubuntu/project/manifest/valid/\" --ext \"wav\" --valid-percent 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /home/ubuntu/project/manifest/valid/valid.tsv /home/ubuntu/project/manifest/train/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/ubuntu/project/manifest/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read transcriptions \n",
    "def generate_transcriptions_dict(transcriptions_path):\n",
    "    all_words = \"\"\n",
    "    with open(transcriptions_path, 'r') as trans:\n",
    "        transcriptions = {}\n",
    "        texts = trans.readlines()\n",
    "        for text in texts:\n",
    "            name, *words = text.strip(\"\\n\").split(\" \")\n",
    "            transcriptions[name] = \" \".join(words)\n",
    "            all_words=all_words+\" \".join(words)\n",
    "    return transcriptions, all_words\n",
    "\n",
    "transcriptions_path = \"/home/ubuntu/espnet/egs2/wsj/asr1/data/test_dev93/text\"\n",
    "valid_trans, all_words = generate_transcriptions_dict(transcriptions_path)\n",
    "\n",
    "transcriptions_path = \"/home/ubuntu/espnet/egs2/wsj/asr1/data/train_si284/text\"\n",
    "train_trans, all_words = generate_transcriptions_dict(transcriptions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate word frequency\n",
    "from collections import Counter\n",
    "wordcount = Counter(all_words)\n",
    "word_count = wordcount.most_common(len(wordcount))\n",
    "with open(output_dir + \"dict.ltr.txt\", 'w') as f:\n",
    "    for word, cnt in word_count:\n",
    "        if word == ' ':\n",
    "            print(\"| {}\".format(cnt), file=f)\n",
    "        else:\n",
    "            print(f\"{word} {cnt}\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def write_labels(tsv_path, output_dir, output_name, transcriptions):\n",
    "\n",
    "    with open(tsv_path, \"r\") as tsv, \\\n",
    "    open(os.path.join(output_dir, output_name + \".ltr\"), \"w\") as ltr_out, \\\n",
    "    open(os.path.join(output_dir, output_name + \".wrd\"), \"w\") as wrd_out:\n",
    "        root = next(tsv).strip()\n",
    "        for line in tsv:\n",
    "            line = line.strip()\n",
    "            name = line.split(\".\")[0]\n",
    "            if name in transcriptions:\n",
    "                trans = transcriptions[name]\n",
    "                print(trans, file=wrd_out)#    \n",
    "                print(\" \".join(trans.replace(\" \", \"|\")) + \" |\",file=ltr_out)\n",
    "            else:\n",
    "                raise Exception(\"missing {} in transcriptions\".format(name))\n",
    "                \n",
    "output_name = \"train\"\n",
    "tsv = output_dir + \"train.tsv\"\n",
    "write_labels(tsv, output_dir, output_name, transcriptions=train_trans)\n",
    "\n",
    "output_name = \"valid\"\n",
    "tsv = output_dir + \"valid.tsv\"\n",
    "write_labels(tsv, output_dir, output_name, transcriptions=valid_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_last.pt  nohup.out  tokens.txt  train.tsv  valid.ltr  valid.wrd\r\n",
      "dict.ltr.txt        \u001b[0m\u001b[01;34moutputs\u001b[0m/   train.ltr   train.wrd  valid.tsv\r\n"
     ]
    }
   ],
   "source": [
    "ls /home/ubuntu/project/manifest/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import os\n",
    "train_dir = \"/home/ubuntu/data/WSJ/train_si284/\"\n",
    "total_duration = 0\n",
    "files = os.listdir(train_dir)\n",
    "for f in files:\n",
    "    sec = soundfile.info(train_dir+f).duration\n",
    "    total_duration += sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_duration//3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-05 07:31:23 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'tensorboard_logdir': None, 'wandb_project': None, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': True}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'broadcast_buffers': False, 'distributed_wrapper': 'DDP', 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3200000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3200000, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [4], 'lr': [3e-05], 'min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': False, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_ctc', 'w2v_path': '/home/ubuntu/project/manifest/train/checkpoint_last.pt', 'apply_mask': True, 'mask_prob': 0.65, 'mask_channel_prob': 0.5, 'mask_channel_length': 64, 'layerdrop': 0.1, 'activation_dropout': 0.1, 'feature_grad_mult': 0.0, 'freeze_finetune_updates': 0}, 'task': {'_name': 'audio_pretraining', 'data': '/home/ubuntu/project/manifest/train/', 'normalize': False, 'labels': 'ltr'}, 'criterion': {'_name': 'ctc', 'zero_infinity': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08}, 'lr_scheduler': {'_name': 'tri_stage', 'phase_ratio': [0.1, 0.4, 0.5], 'final_lr_scale': 0.05}, 'scoring': None, 'bpe': None, 'tokenizer': None}\n",
      "2020-12-05 07:31:23 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 503, skipped 0 samples\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/psinet/bin/fairseq-hydra-train\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-hydra-train')())\n",
      "  File \"/home/ubuntu/fairseq/fairseq_cli/hydra_train.py\", line 66, in cli_main\n",
      "    hydra_main()\n",
      "  File \"/home/ubuntu/anaconda3/envs/psinet/lib/python3.7/site-packages/hydra/main.py\", line 37, in decorated_main\n",
      "    strict=strict,\n",
      "  File \"/home/ubuntu/anaconda3/envs/psinet/lib/python3.7/site-packages/hydra/_internal/utils.py\", line 347, in _run_hydra\n",
      "    lambda: hydra.run(\n",
      "  File \"/home/ubuntu/anaconda3/envs/psinet/lib/python3.7/site-packages/hydra/_internal/utils.py\", line 201, in run_and_report\n",
      "    raise ex\n",
      "  File \"/home/ubuntu/anaconda3/envs/psinet/lib/python3.7/site-packages/hydra/_internal/utils.py\", line 198, in run_and_report\n",
      "    return func()\n",
      "  File \"/home/ubuntu/anaconda3/envs/psinet/lib/python3.7/site-packages/hydra/_internal/utils.py\", line 350, in <lambda>\n",
      "    overrides=args.overrides,\n",
      "  File \"/home/ubuntu/anaconda3/envs/psinet/lib/python3.7/site-packages/hydra/_internal/hydra.py\", line 112, in run\n",
      "    configure_logging=with_log_configuration,\n",
      "  File \"/home/ubuntu/anaconda3/envs/psinet/lib/python3.7/site-packages/hydra/core/utils.py\", line 125, in run_job\n",
      "    ret.return_value = task_function(task_cfg)\n",
      "  File \"/home/ubuntu/fairseq/fairseq_cli/hydra_train.py\", line 38, in hydra_main\n",
      "    distributed_utils.call_main(cfg, pre_main)\n",
      "  File \"/home/ubuntu/fairseq/fairseq/distributed_utils.py\", line 334, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 74, in main\n",
      "    model = task.build_model(cfg.model)\n",
      "  File \"/home/ubuntu/fairseq/fairseq/tasks/audio_pretraining.py\", line 201, in build_model\n",
      "    model = super().build_model(model_cfg)\n",
      "  File \"/home/ubuntu/fairseq/fairseq/tasks/fairseq_task.py\", line 282, in build_model\n",
      "    model = models.build_model(cfg, self)\n",
      "  File \"/home/ubuntu/fairseq/fairseq/models/__init__.py\", line 86, in build_model\n",
      "    return model.build_model(cfg, task)\n",
      "  File \"/home/ubuntu/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 147, in build_model\n",
      "    w2v_encoder = Wav2VecEncoder(cfg, task.target_dictionary)\n",
      "  File \"/home/ubuntu/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 304, in __init__\n",
      "    model.remove_pretraining_modules()\n",
      "  File \"/home/ubuntu/anaconda3/envs/psinet/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 576, in __getattr__\n",
      "    type(self).__name__, name))\n",
      "AttributeError: 'Wav2VecCtc' object has no attribute 'remove_pretraining_modules'\n",
      "2020-12-05 07:31:27 | INFO | wandb.sdk.internal.internal | Internal process exited\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/project/notebook'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-05 08:04:51 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'tensorboard_logdir': None, 'wandb_project': None, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': True}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'broadcast_buffers': False, 'distributed_wrapper': 'DDP', 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3200000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3200000, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [4], 'lr': [3e-05], 'min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/ubuntu/project/manifest/train/outputs/2020-12-04/06-26-12/checkpoints/', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': False, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_ctc', 'w2v_path': '/home/ubuntu/project/model/wav2vec_small.pt', 'apply_mask': True, 'mask_prob': 0.65, 'mask_channel_prob': 0.5, 'mask_channel_length': 64, 'layerdrop': 0.1, 'activation_dropout': 0.1, 'feature_grad_mult': 0.0, 'freeze_finetune_updates': 0}, 'task': {'_name': 'audio_pretraining', 'data': '/home/ubuntu/project/manifest/train/', 'normalize': False, 'labels': 'ltr'}, 'criterion': {'_name': 'ctc', 'zero_infinity': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08}, 'lr_scheduler': {'_name': 'tri_stage', 'phase_ratio': [0.1, 0.4, 0.5], 'final_lr_scale': 0.05}, 'scoring': None, 'bpe': None, 'tokenizer': None}\n",
      "2020-12-05 08:04:51 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 503, skipped 0 samples\n",
      "2020-12-05 08:04:55 | INFO | fairseq_cli.train | Wav2VecCtc(\n",
      "  (w2v_encoder): Wav2VecEncoder(\n",
      "    (w2v_model): Wav2Vec2Model(\n",
      "      (feature_extractor): ConvFeatureExtractionModel(\n",
      "        (conv_layers): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "            (3): GELU()\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (4): Sequential(\n",
      "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (5): Sequential(\n",
      "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (6): Sequential(\n",
      "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): GELU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
      "      (dropout_input): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_features): Dropout(p=0.1, inplace=False)\n",
      "      (quantizer): None\n",
      "      (project_q): None\n",
      "      (encoder): TransformerEncoder(\n",
      "        (pos_conv): Sequential(\n",
      "          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
      "          (1): SamePad()\n",
      "          (2): GELU()\n",
      "        )\n",
      "        (layers): ModuleList(\n",
      "          (0): TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (1): TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (3): TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (4): TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (5): TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (6): TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (7): TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (8): TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (9): TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (10): TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (11): TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (final_proj): None\n",
      "    )\n",
      "    (final_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=768, out_features=52, bias=True)\n",
      "  )\n",
      ")\n",
      "2020-12-05 08:04:55 | INFO | fairseq_cli.train | task: AudioPretrainingTask\n",
      "2020-12-05 08:04:55 | INFO | fairseq_cli.train | model: Wav2VecCtc\n",
      "2020-12-05 08:04:55 | INFO | fairseq_cli.train | criterion: CtcCriterion)\n",
      "2020-12-05 08:04:55 | INFO | fairseq_cli.train | num. model params: 94411700 (num. trained: 94411700)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-05 08:04:58 | INFO | fairseq.trainer | detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias\n",
      "2020-12-05 08:04:58 | INFO | fairseq.trainer | detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias\n",
      "2020-12-05 08:04:58 | INFO | fairseq.trainer | detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias\n",
      "2020-12-05 08:04:58 | INFO | fairseq.trainer | detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias\n",
      "2020-12-05 08:04:58 | INFO | fairseq.trainer | detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias\n",
      "2020-12-05 08:04:58 | INFO | fairseq.trainer | detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias\n",
      "2020-12-05 08:04:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2020-12-05 08:04:58 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
      "2020-12-05 08:04:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2020-12-05 08:04:58 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2020-12-05 08:04:58 | INFO | fairseq_cli.train | max tokens per GPU = 3200000 and batch size per GPU = None\n",
      "2020-12-05 08:05:04 | INFO | fairseq.trainer | loaded checkpoint /home/ubuntu/project/manifest/train/outputs/2020-12-04/06-26-12/checkpoints/checkpoint_last.pt (epoch 28 @ 31579 updates)\n",
      "2020-12-05 08:05:05 | INFO | fairseq.trainer | loading train data for epoch 28\n",
      "2020-12-05 08:05:05 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 37416, skipped 0 samples\n",
      "2020-12-05 08:05:05 | INFO | fairseq.trainer | begin training epoch 28\n",
      "2020-12-05 08:05:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 648.00 MiB (GPU 0; 14.76 GiB total capacity; 3.35 GiB already allocated; 228.75 MiB free; 6.01 GiB reserved in total by PyTorch)\n",
      "2020-12-05 08:05:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    2786 MB |    5645 MB |   89132 MB |   86345 MB |\n",
      "|       from large pool |    2782 MB |    5641 MB |   89097 MB |   86314 MB |\n",
      "|       from small pool |       4 MB |       6 MB |      34 MB |      30 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    2786 MB |    5645 MB |   89132 MB |   86345 MB |\n",
      "|       from large pool |    2782 MB |    5641 MB |   89097 MB |   86314 MB |\n",
      "|       from small pool |       4 MB |       6 MB |      34 MB |      30 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    6152 MB |    6154 MB |    7296 MB |    1144 MB |\n",
      "|       from large pool |    6146 MB |    6146 MB |    7288 MB |    1142 MB |\n",
      "|       from small pool |       6 MB |       8 MB |       8 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2717 MB |    3047 MB |   87926 MB |   85208 MB |\n",
      "|       from large pool |    2715 MB |    3045 MB |   87887 MB |   85171 MB |\n",
      "|       from small pool |       1 MB |       2 MB |      39 MB |      37 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     397    |     625    |    5751    |    5354    |\n",
      "|       from large pool |     153    |     316    |    3363    |    3210    |\n",
      "|       from small pool |     244    |     310    |    2388    |    2144    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     397    |     625    |    5751    |    5354    |\n",
      "|       from large pool |     153    |     316    |    3363    |    3210    |\n",
      "|       from small pool |     244    |     310    |    2388    |    2144    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      38    |      55    |      58    |      20    |\n",
      "|       from large pool |      35    |      52    |      54    |      19    |\n",
      "|       from small pool |       3    |       4    |       4    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      34    |      40    |    2620    |    2586    |\n",
      "|       from large pool |      27    |      36    |    1931    |    1904    |\n",
      "|       from small pool |       7    |      13    |     689    |     682    |\n",
      "|===========================================================================|\n",
      "\n",
      "2020-12-05 08:05:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
      "2020-12-05 08:05:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 648.00 MiB (GPU 0; 14.76 GiB total capacity; 3.37 GiB already allocated; 26.75 MiB free; 6.21 GiB reserved in total by PyTorch)\n",
      "2020-12-05 08:05:21 | WARNING | fairseq.trainer | |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    2801 MB |    5819 MB |  189031 MB |  186230 MB |\n",
      "|       from large pool |    2796 MB |    5815 MB |  188962 MB |  186166 MB |\n",
      "|       from small pool |       4 MB |       6 MB |      69 MB |      64 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    2801 MB |    5819 MB |  189031 MB |  186230 MB |\n",
      "|       from large pool |    2796 MB |    5815 MB |  188962 MB |  186166 MB |\n",
      "|       from small pool |       4 MB |       6 MB |      69 MB |      64 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    6354 MB |    6356 MB |   11594 MB |    5240 MB |\n",
      "|       from large pool |    6348 MB |    6348 MB |   11582 MB |    5234 MB |\n",
      "|       from small pool |       6 MB |       8 MB |      12 MB |       6 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2904 MB |    3233 MB |  190586 MB |  187681 MB |\n",
      "|       from large pool |    2903 MB |    3232 MB |  190508 MB |  187605 MB |\n",
      "|       from small pool |       1 MB |       2 MB |      78 MB |      76 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     411    |     646    |   12213    |   11802    |\n",
      "|       from large pool |     159    |     329    |    7235    |    7076    |\n",
      "|       from small pool |     252    |     318    |    4978    |    4726    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     411    |     646    |   12213    |   11802    |\n",
      "|       from large pool |     159    |     329    |    7235    |    7076    |\n",
      "|       from small pool |     252    |     318    |    4978    |    4726    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      37    |      55    |      92    |      55    |\n",
      "|       from large pool |      34    |      52    |      86    |      52    |\n",
      "|       from small pool |       3    |       4    |       6    |       3    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      40    |    5371    |    5335    |\n",
      "|       from large pool |      28    |      36    |    3984    |    3956    |\n",
      "|       from small pool |       8    |      13    |    1387    |    1379    |\n",
      "|===========================================================================|\n",
      "\n",
      "2020-12-05 08:05:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-05 08:05:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 648.00 MiB (GPU 0; 14.76 GiB total capacity; 3.35 GiB already allocated; 58.75 MiB free; 6.17 GiB reserved in total by PyTorch)\n",
      "2020-12-05 08:05:39 | WARNING | fairseq.trainer | |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 3            |        cudaMalloc retries: 6         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    2786 MB |    5819 MB |  426174 MB |  423387 MB |\n",
      "|       from large pool |    2782 MB |    5815 MB |  426019 MB |  423237 MB |\n",
      "|       from small pool |       4 MB |       6 MB |     154 MB |     150 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    2786 MB |    5819 MB |  426174 MB |  423387 MB |\n",
      "|       from large pool |    2782 MB |    5815 MB |  426019 MB |  423237 MB |\n",
      "|       from small pool |       4 MB |       6 MB |     154 MB |     150 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    6322 MB |    6356 MB |   15892 MB |    9570 MB |\n",
      "|       from large pool |    6316 MB |    6348 MB |   15876 MB |    9560 MB |\n",
      "|       from small pool |       6 MB |       8 MB |      16 MB |      10 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2887 MB |    3233 MB |  442222 MB |  439335 MB |\n",
      "|       from large pool |    2885 MB |    3232 MB |  442048 MB |  439163 MB |\n",
      "|       from small pool |       1 MB |       2 MB |     173 MB |     171 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     397    |     646    |   27481    |   27084    |\n",
      "|       from large pool |     153    |     329    |   16327    |   16174    |\n",
      "|       from small pool |     244    |     318    |   11154    |   10910    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     397    |     646    |   27481    |   27084    |\n",
      "|       from large pool |     153    |     329    |   16327    |   16174    |\n",
      "|       from small pool |     244    |     318    |   11154    |   10910    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      36    |      55    |     126    |      90    |\n",
      "|       from large pool |      33    |      52    |     118    |      85    |\n",
      "|       from small pool |       3    |       4    |       8    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      38    |      41    |   12505    |   12467    |\n",
      "|       from large pool |      27    |      36    |    9393    |    9366    |\n",
      "|       from small pool |      11    |      16    |    3112    |    3101    |\n",
      "|===========================================================================|\n",
      "\n",
      "2020-12-05 08:05:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
      "2020-12-05 08:05:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 648.00 MiB (GPU 0; 14.76 GiB total capacity; 3.37 GiB already allocated; 642.75 MiB free; 5.60 GiB reserved in total by PyTorch)\n",
      "2020-12-05 08:05:41 | WARNING | fairseq.trainer | |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 8         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    2800 MB |    5819 MB |  447636 MB |  444835 MB |\n",
      "|       from large pool |    2796 MB |    5815 MB |  447475 MB |  444678 MB |\n",
      "|       from small pool |       4 MB |       6 MB |     161 MB |     157 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    2800 MB |    5819 MB |  447636 MB |  444835 MB |\n",
      "|       from large pool |    2796 MB |    5815 MB |  447475 MB |  444678 MB |\n",
      "|       from small pool |       4 MB |       6 MB |     161 MB |     157 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    5738 MB |    6356 MB |   19478 MB |   13740 MB |\n",
      "|       from large pool |    5732 MB |    6348 MB |   19460 MB |   13728 MB |\n",
      "|       from small pool |       6 MB |       8 MB |      18 MB |      12 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2289 MB |    3233 MB |  463146 MB |  460857 MB |\n",
      "|       from large pool |    2287 MB |    3232 MB |  462963 MB |  460676 MB |\n",
      "|       from small pool |       1 MB |       2 MB |     182 MB |     180 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     411    |     646    |   28796    |   28385    |\n",
      "|       from large pool |     159    |     329    |   17117    |   16958    |\n",
      "|       from small pool |     252    |     318    |   11679    |   11427    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     411    |     646    |   28796    |   28385    |\n",
      "|       from large pool |     159    |     329    |   17117    |   16958    |\n",
      "|       from small pool |     252    |     318    |   11679    |   11427    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      37    |      55    |     157    |     120    |\n",
      "|       from large pool |      34    |      52    |     148    |     114    |\n",
      "|       from small pool |       3    |       4    |       9    |       6    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      41    |   13121    |   13085    |\n",
      "|       from large pool |      26    |      36    |    9894    |    9868    |\n",
      "|       from small pool |      10    |      16    |    3227    |    3217    |\n",
      "|===========================================================================|\n",
      "\n",
      "2020-12-05 08:05:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
      "2020-12-05 08:06:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 648.00 MiB (GPU 0; 14.76 GiB total capacity; 3.35 GiB already allocated; 28.75 MiB free; 6.20 GiB reserved in total by PyTorch)\n",
      "2020-12-05 08:06:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    2786 MB |    5822 MB |     824 GB |     821 GB |\n",
      "|       from large pool |    2782 MB |    5817 MB |     824 GB |     821 GB |\n",
      "|       from small pool |       4 MB |       6 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    2786 MB |    5822 MB |     824 GB |     821 GB |\n",
      "|       from large pool |    2782 MB |    5817 MB |     824 GB |     821 GB |\n",
      "|       from small pool |       4 MB |       6 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    6352 MB |    6356 MB |   23744 MB |   17392 MB |\n",
      "|       from large pool |    6346 MB |    6348 MB |   23722 MB |   17376 MB |\n",
      "|       from small pool |       6 MB |       8 MB |      22 MB |      16 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2917 MB |    3246 MB |     862 GB |     859 GB |\n",
      "|       from large pool |    2915 MB |    3244 MB |     861 GB |     858 GB |\n",
      "|       from small pool |       1 MB |       2 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     397    |     646    |   54544    |   54147    |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       from large pool |     153    |     329    |   32491    |   32338    |\n",
      "|       from small pool |     244    |     318    |   22053    |   21809    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     397    |     646    |   54544    |   54147    |\n",
      "|       from large pool |     153    |     329    |   32491    |   32338    |\n",
      "|       from small pool |     244    |     318    |   22053    |   21809    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      36    |      55    |     190    |     154    |\n",
      "|       from large pool |      33    |      52    |     179    |     146    |\n",
      "|       from small pool |       3    |       4    |      11    |       8    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      32    |      49    |   25675    |   25643    |\n",
      "|       from large pool |      26    |      36    |   18751    |   18725    |\n",
      "|       from small pool |       6    |      20    |    6924    |    6918    |\n",
      "|===========================================================================|\n",
      "\n",
      "2020-12-05 08:06:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
      "2020-12-05 08:06:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 648.00 MiB (GPU 0; 14.76 GiB total capacity; 3.35 GiB already allocated; 222.75 MiB free; 6.01 GiB reserved in total by PyTorch)\n",
      "2020-12-05 08:06:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 6            |        cudaMalloc retries: 12        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    2785 MB |    5822 MB |     919 GB |     916 GB |\n",
      "|       from large pool |    2781 MB |    5817 MB |     919 GB |     916 GB |\n",
      "|       from small pool |       4 MB |       6 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    2785 MB |    5822 MB |     919 GB |     916 GB |\n",
      "|       from large pool |    2781 MB |    5817 MB |     919 GB |     916 GB |\n",
      "|       from small pool |       4 MB |       6 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    6158 MB |    6356 MB |   27846 MB |   21688 MB |\n",
      "|       from large pool |    6152 MB |    6348 MB |   27820 MB |   21668 MB |\n",
      "|       from small pool |       6 MB |       8 MB |      26 MB |      20 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2724 MB |    3246 MB |     956 GB |     954 GB |\n",
      "|       from large pool |    2722 MB |    3244 MB |     956 GB |     953 GB |\n",
      "|       from small pool |       1 MB |       2 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     397    |     646    |   60820    |   60423    |\n",
      "|       from large pool |     153    |     329    |   36245    |   36092    |\n",
      "|       from small pool |     244    |     318    |   24575    |   24331    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     397    |     646    |   60820    |   60423    |\n",
      "|       from large pool |     153    |     329    |   36245    |   36092    |\n",
      "|       from small pool |     244    |     318    |   24575    |   24331    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      43    |      55    |     230    |     187    |\n",
      "|       from large pool |      40    |      52    |     217    |     177    |\n",
      "|       from small pool |       3    |       4    |      13    |      10    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      40    |      49    |   28778    |   28738    |\n",
      "|       from large pool |      31    |      36    |   21061    |   21030    |\n",
      "|       from small pool |       9    |      20    |    7717    |    7708    |\n",
      "|===========================================================================|\n",
      "\n",
      "2020-12-05 08:06:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
      "2020-12-05 08:06:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 648.00 MiB (GPU 0; 14.76 GiB total capacity; 2.73 GiB already allocated; 642.75 MiB free; 5.60 GiB reserved in total by PyTorch)\n",
      "2020-12-05 08:06:21 | WARNING | fairseq.trainer | |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 7            |        cudaMalloc retries: 14        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    2795 MB |    5822 MB |     939 GB |     937 GB |\n",
      "|       from large pool |    2791 MB |    5817 MB |     939 GB |     936 GB |\n",
      "|       from small pool |       4 MB |       6 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    2795 MB |    5822 MB |     939 GB |     937 GB |\n",
      "|       from large pool |    2791 MB |    5817 MB |     939 GB |     936 GB |\n",
      "|       from small pool |       4 MB |       6 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    5738 MB |    6356 MB |   31462 MB |   25724 MB |\n",
      "|       from large pool |    5732 MB |    6348 MB |   31434 MB |   25702 MB |\n",
      "|       from small pool |       6 MB |       8 MB |      28 MB |      22 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2942 MB |    3271 MB |     977 GB |     974 GB |\n",
      "|       from large pool |    2940 MB |    3269 MB |     976 GB |     973 GB |\n",
      "|       from small pool |       1 MB |       2 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     411    |     646    |   62132    |   61721    |\n",
      "|       from large pool |     159    |     329    |   37034    |   36875    |\n",
      "|       from small pool |     252    |     318    |   25098    |   24846    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     411    |     646    |   62132    |   61721    |\n",
      "|       from large pool |     159    |     329    |   37034    |   36875    |\n",
      "|       from small pool |     252    |     318    |   25098    |   24846    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      37    |      55    |     261    |     224    |\n",
      "|       from large pool |      34    |      52    |     247    |     213    |\n",
      "|       from small pool |       3    |       4    |      14    |      11    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      33    |      49    |   29456    |   29423    |\n",
      "|       from large pool |      26    |      36    |   21551    |   21525    |\n",
      "|       from small pool |       7    |      20    |    7905    |    7898    |\n",
      "|===========================================================================|\n",
      "\n",
      "2020-12-05 08:06:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
      "2020-12-05 08:06:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 648.00 MiB (GPU 0; 14.76 GiB total capacity; 2.73 GiB already allocated; 642.75 MiB free; 5.60 GiB reserved in total by PyTorch)\n",
      "2020-12-05 08:06:23 | WARNING | fairseq.trainer | |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 8            |        cudaMalloc retries: 16        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    2795 MB |    5822 MB |     960 GB |     957 GB |\n",
      "|       from large pool |    2791 MB |    5817 MB |     959 GB |     957 GB |\n",
      "|       from small pool |       4 MB |       6 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    2795 MB |    5822 MB |     960 GB |     957 GB |\n",
      "|       from large pool |    2791 MB |    5817 MB |     959 GB |     957 GB |\n",
      "|       from small pool |       4 MB |       6 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    5738 MB |    6356 MB |   35078 MB |   29340 MB |\n",
      "|       from large pool |    5732 MB |    6348 MB |   35048 MB |   29316 MB |\n",
      "|       from small pool |       6 MB |       8 MB |      30 MB |      24 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2942 MB |    3271 MB |     997 GB |     994 GB |\n",
      "|       from large pool |    2940 MB |    3269 MB |     996 GB |     993 GB |\n",
      "|       from small pool |       1 MB |       2 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     411    |     646    |   63444    |   63033    |\n",
      "|       from large pool |     159    |     329    |   37823    |   37664    |\n",
      "|       from small pool |     252    |     318    |   25621    |   25369    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     411    |     646    |   63444    |   63033    |\n",
      "|       from large pool |     159    |     329    |   37823    |   37664    |\n",
      "|       from small pool |     252    |     318    |   25621    |   25369    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      37    |      55    |     292    |     255    |\n",
      "|       from large pool |      34    |      52    |     277    |     243    |\n",
      "|       from small pool |       3    |       4    |      15    |      12    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      32    |      49    |   30141    |   30109    |\n",
      "|       from large pool |      26    |      36    |   22043    |   22017    |\n",
      "|       from small pool |       6    |      20    |    8098    |    8092    |\n",
      "|===========================================================================|\n",
      "\n",
      "2020-12-05 08:06:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-05 08:06:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 648.00 MiB (GPU 0; 14.76 GiB total capacity; 3.37 GiB already allocated; 96.75 MiB free; 6.14 GiB reserved in total by PyTorch)\n",
      "2020-12-05 08:06:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 9            |        cudaMalloc retries: 17        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    2802 MB |    5822 MB |    1073 GB |    1070 GB |\n",
      "|       from large pool |    2798 MB |    5817 MB |    1073 GB |    1070 GB |\n",
      "|       from small pool |       4 MB |       6 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    2802 MB |    5822 MB |    1073 GB |    1070 GB |\n",
      "|       from large pool |    2798 MB |    5817 MB |    1073 GB |    1070 GB |\n",
      "|       from small pool |       4 MB |       6 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    6284 MB |    6356 MB |   38352 MB |   32068 MB |\n",
      "|       from large pool |    6278 MB |    6348 MB |   38318 MB |   32040 MB |\n",
      "|       from small pool |       6 MB |       8 MB |      34 MB |      28 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2833 MB |    3271 MB |    1107 GB |    1104 GB |\n",
      "|       from large pool |    2831 MB |    3269 MB |    1106 GB |    1104 GB |\n",
      "|       from small pool |       1 MB |       2 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     411    |     646    |   70929    |   70518    |\n",
      "|       from large pool |     159    |     329    |   42302    |   42143    |\n",
      "|       from small pool |     252    |     318    |   28627    |   28375    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     411    |     646    |   70929    |   70518    |\n",
      "|       from large pool |     159    |     329    |   42302    |   42143    |\n",
      "|       from small pool |     252    |     318    |   28627    |   28375    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      45    |      55    |     318    |     273    |\n",
      "|       from large pool |      42    |      52    |     301    |     259    |\n",
      "|       from small pool |       3    |       4    |      17    |      14    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      45    |      49    |   33839    |   33794    |\n",
      "|       from large pool |      34    |      36    |   24692    |   24658    |\n",
      "|       from small pool |      11    |      20    |    9147    |    9136    |\n",
      "|===========================================================================|\n",
      "\n",
      "2020-12-05 08:06:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
      "2020-12-05 08:06:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 648.00 MiB (GPU 0; 14.76 GiB total capacity; 3.34 GiB already allocated; 640.75 MiB free; 5.61 GiB reserved in total by PyTorch)\n",
      "2020-12-05 08:06:39 | WARNING | fairseq.trainer | |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 10           |        cudaMalloc retries: 19        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    2769 MB |    5822 MB |    1166 GB |    1163 GB |\n",
      "|       from large pool |    2764 MB |    5817 MB |    1165 GB |    1163 GB |\n",
      "|       from small pool |       4 MB |       6 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    2769 MB |    5822 MB |    1166 GB |    1163 GB |\n",
      "|       from large pool |    2764 MB |    5817 MB |    1165 GB |    1163 GB |\n",
      "|       from small pool |       4 MB |       6 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    5740 MB |    6356 MB |   41940 MB |   36200 MB |\n",
      "|       from large pool |    5734 MB |    6348 MB |   41902 MB |   36168 MB |\n",
      "|       from small pool |       6 MB |       8 MB |      38 MB |      32 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2322 MB |    3271 MB |    1209 GB |    1207 GB |\n",
      "|       from large pool |    2321 MB |    3269 MB |    1208 GB |    1206 GB |\n",
      "|       from small pool |       1 MB |       2 MB |       0 GB |       0 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     383    |     646    |   77019    |   76636    |\n",
      "|       from large pool |     147    |     329    |   45938    |   45791    |\n",
      "|       from small pool |     236    |     318    |   31081    |   30845    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     383    |     646    |   77019    |   76636    |\n",
      "|       from large pool |     147    |     329    |   45938    |   45791    |\n",
      "|       from small pool |     236    |     318    |   31081    |   30845    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      38    |      55    |     350    |     312    |\n",
      "|       from large pool |      35    |      52    |     331    |     296    |\n",
      "|       from small pool |       3    |       4    |      19    |      16    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      32    |      53    |   36741    |   36709    |\n",
      "|       from large pool |      27    |      37    |   26849    |   26822    |\n",
      "|       from small pool |       5    |      20    |    9892    |    9887    |\n",
      "|===========================================================================|\n",
      "\n",
      "2020-12-05 08:06:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n"
     ]
    }
   ],
   "source": [
    "!env HYDRA_FULL_ERROR=1 fairseq-hydra-train \\\n",
    "    task.data=/home/ubuntu/project/manifest/train/ \\\n",
    "    model.w2v_path=/home/ubuntu/project/model/wav2vec_small.pt \\\n",
    "    --config-dir /home/ubuntu/fairseq/examples/wav2vec/config/finetuning \\\n",
    "    --config-name base_100h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
